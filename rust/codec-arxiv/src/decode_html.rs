//! HTML decoder for arXiv LaTeXML-generated content.
//!
//! This module handles the conversion of HTML generated by LaTeXML from arXiv papers
//! into Stencila document structures. LaTeXML produces semantic HTML with specific
//! CSS classes (prefixed with "ltx_") that indicate the original LaTeX structure.

use std::{path::Path, str::FromStr, sync::LazyLock};

use regex::Regex;
use tl::{HTMLTag, Parser, ParserOptions, parse};
use tokio::fs::read_to_string;

use stencila_codec::{
    DecodeInfo, DecodeOptions, Losses,
    eyre::{Result, bail},
    stencila_schema::{Article, Author, Block, Inline, Node, Person, Reference, shortcuts::t},
};
use stencila_codec_biblio::decode::text_to_reference;

use super::decode::arxiv_id_to_doi;
use super::decode_html_blocks::*;
use super::decode_html_inlines::*;

/// Decode LaTeXML-generated HTML from arXiv to a Stencila [`Node`]
///
/// This function processes HTML generated by LaTeXML (https://github.com/brucemiller/LaTeXML)
/// from arXiv papers. The HTML contains semantic markup with CSS classes that preserve
/// the original LaTeX document structure.
///
/// # Arguments
/// * `arxiv_id` - The arXiv identifier (e.g., "2301.00001")
/// * `path` - Path to the HTML file from arXiv's export service
/// * `_options` - Decode options (currently unused)
///
/// # Returns
/// A tuple of (Node, DecodeInfo) where Node contains the structured document
/// and DecodeInfo tracks any parsing losses or issues.
#[tracing::instrument(skip(_options))]
pub async fn decode_arxiv_html(
    arxiv_id: &str,
    path: &Path,
    _options: Option<DecodeOptions>,
) -> Result<(Node, DecodeInfo)> {
    let html = read_to_string(path).await?;

    if html.trim().is_empty() {
        bail!("Retrieved HTML content is empty");
    }

    // Parse the HTML
    let dom = parse(&html, ParserOptions::default())?;
    let parser = dom.parser();

    // Extract the <article> element (ignore <nav> and <footer> content)
    let Some(article) = dom
        .query_selector("article")
        .and_then(|mut nodes| nodes.next())
        .and_then(|article| article.get(parser))
        .and_then(|article| article.as_tag())
    else {
        bail!("No <article> tag in HTML")
    };

    // Extract <base> href from head for resolving relative URLs
    let base_href = dom
        .query_selector("base[href]")
        .and_then(|mut nodes| nodes.next())
        .and_then(|node| node.get(parser))
        .and_then(|node| node.as_tag())
        .and_then(|node| get_attr(node, "href"));

    // Decode article
    let mut context = ArxivDecodeContext::new(base_href);
    let mut article = decode_article(parser, article, &mut context);

    // Set DOI, and other metadata
    if !arxiv_id.is_empty() {
        article.doi = Some(arxiv_id_to_doi(arxiv_id));
        article.options.repository = Some("https://arxiv.org".into());
        article.options.path = Some(["html/", arxiv_id].concat());
    }

    Ok((
        Node::Article(article),
        DecodeInfo {
            losses: context.losses,
            ..Default::default()
        },
    ))
}

pub struct ArxivDecodeContext {
    losses: Losses,
    pub appendix_started: bool,
    pub base_href: Option<String>,
}

impl ArxivDecodeContext {
    pub fn new(base_href: Option<String>) -> Self {
        Self {
            losses: Losses::none(),
            appendix_started: false,
            base_href,
        }
    }

    /// Resolve relative URLs using base href from LaTeXML output
    ///
    /// LaTeXML often generates relative URLs for figures and links that need to be
    /// resolved against the export.arxiv.org base to create working absolute URLs.
    /// The base href is typically extracted from the HTML document's <base> tag.
    pub fn resolve_url(&self, url: &str) -> String {
        // Already absolute URLs (http/https/data) don't need resolution
        if url.starts_with("http://") || url.starts_with("https://") || url.starts_with("data:") {
            return url.to_string();
        }

        // Resolve relative URLs using base href from document <base> tag
        if let Some(base_href) = &self.base_href {
            // Clean up base path by removing leading/trailing slashes
            let base = base_href.trim_start_matches('/').trim_end_matches('/');
            let relative_url = url.trim_start_matches('/');

            format!("https://export.arxiv.org/{base}/{relative_url}")
        } else {
            // No base available - this might indicate a parsing issue but handle gracefully
            tracing::warn!("No base href available for resolving relative URL: {}", url);
            url.to_string()
        }
    }

    pub fn add_loss(&mut self, tag: &HTMLTag) {
        let tag_name = tag.name().as_utf8_str();

        let class = tag
            .attributes()
            .class()
            .map(|cls| format!(" class=\"{}\"", cls.as_utf8_str()))
            .unwrap_or_default();

        self.losses.add(format!("<{tag_name}{class}>",))
    }
}

// Helper functions for common HTML attribute extraction patterns

/// Get the class attribute as a string, or empty string if not present
pub fn get_class(tag: &HTMLTag) -> String {
    tag.attributes()
        .class()
        .map(|cls| cls.as_utf8_str().to_string())
        .unwrap_or_default()
}

/// Get an attribute value as Option<String>
pub fn get_attr(tag: &HTMLTag, name: &str) -> Option<String> {
    tag.attributes()
        .get(name)
        .flatten()
        .map(|bytes| bytes.as_utf8_str().to_string())
}

/// Get href attribute, extracting hash fragment for internal links
pub fn get_href_target(tag: &HTMLTag) -> Option<String> {
    get_attr(tag, "href").map(|href| {
        // For internal links, extract only the hash part
        if let Some(hash_pos) = href.find('#') {
            href[hash_pos + 1..].to_string()
        } else if let Some(rest) = href.strip_prefix('#') {
            rest.to_string()
        } else {
            // External link, keep full URL
            href
        }
    })
}

/// Decode common HTML entities
pub fn decode_html_entities(text: &str) -> String {
    text.replace("&amp;", "&")
        .replace("&lt;", "<")
        .replace("&gt;", ">")
        .replace("&quot;", "\"")
        .replace("&#x27;", "'")
        .replace("&#39;", "'")
        .replace("&apos;", "'")
        .replace("&nbsp;", " ")
}

/// Check if a text string is an email address or contains email pattern
fn is_email_address(text: &str) -> bool {
    let trimmed = text.trim();

    // Check for @ symbol
    if !trimmed.contains('@') {
        return false;
    }

    // Find @ position
    if let Some(at_pos) = trimmed.find('@') {
        let after_at = &trimmed[at_pos + 1..];
        // Must have a dot after @ and reasonable length
        if after_at.contains('.') && after_at.len() > 3 {
            return true;
        }
    }

    false
}

/// Check if text contains structured email patterns like {name1, name2}@domain
///
/// This function detects patterns where email usernames are listed in braces,
/// which LaTeXML often uses for contact information. These patterns should not
/// be parsed as individual author names.
fn contains_structured_email_pattern(text: &str) -> bool {
    // Look for patterns like {name1, name2, name3}@domain
    let structured_email_regex =
        Regex::new(r"\{[^}]+\}@[^\s]+").expect("valid email pattern regex");
    structured_email_regex.is_match(text)
}

/// Check if a single text part is likely a username from a structured email
///
/// This detects individual components that were extracted from patterns like
/// {daniel, brendan, parth}@domain.com where "daniel", "brendan", "parth"
/// should not be treated as separate authors.
fn is_email_username_part(text: &str, full_text: &str) -> bool {
    let trimmed = text.trim().to_lowercase();

    // Skip very short text that could be usernames
    if trimmed.len() <= 2 {
        return true;
    }

    // Check if this appears to be part of a structured email pattern
    if contains_structured_email_pattern(full_text) {
        // Look for this text followed by email-like patterns in the full text
        let pattern = format!(r"(?i)\b{}\b[^@]*@", regex::escape(&trimmed));
        if let Ok(re) = Regex::new(&pattern)
            && re.is_match(full_text)
        {
            return true;
        }

        // Also check if it looks like common email username patterns
        if trimmed.chars().all(|c| c.is_ascii_lowercase()) && trimmed.len() < 12 {
            return true;
        }
    }

    false
}

/// Check if a text string is an organization/institution name
///
/// This function is more conservative than before to avoid filtering out
/// legitimate author names that might contain institutional keywords.
fn is_organization_name(text: &str) -> bool {
    let trimmed = text.trim().to_lowercase();

    // Skip very short strings (likely abbreviations or names)
    if trimmed.len() < 4 {
        return false;
    }

    // Be more conservative - only filter obvious institutional patterns
    // that are unlikely to be person names

    // Check for exact organizational standalone words (very conservative)
    if trimmed == "department"
        || trimmed == "university"
        || trimmed == "institute"
        || trimmed == "college"
        || trimmed == "school"
        || trimmed == "laboratory"
        || trimmed == "research"
        || trimmed == "foundation"
        || trimmed == "organization"
        || trimmed == "organisation"
        || trimmed == "company"
        || trimmed == "corporation"
        || trimmed == "technology"  // Add this based on the test case
        || trimmed == "engineering" // Add this based on the test case
        || trimmed == "management"
    // Add this based on the test case
    {
        return true;
    }

    // Check for clear organizational prefixes (more specific than before)
    if trimmed.starts_with("department of ")
        || trimmed.starts_with("university of ")
        || trimmed.starts_with("institute of ")
        || trimmed.starts_with("school of ")
        || trimmed.starts_with("college of ")
        || trimmed.starts_with("center of ")
        || trimmed.starts_with("centre of ")
        || trimmed.starts_with("faculty of ")
    {
        return true;
    }

    // Check for organizational endings but be more specific
    if trimmed.len() > 8
        && (
            // Only for longer strings
            trimmed.ends_with(" university")
                || trimmed.ends_with(" institute")
                || trimmed.ends_with(" college")
                || trimmed.ends_with(" laboratory")
                || trimmed.ends_with(" foundation")
                || trimmed.ends_with(" corporation")
                || trimmed.ends_with(" inc.")
                || trimmed.ends_with(" llc")
                || trimmed.ends_with(" ltd.")
                || trimmed.ends_with(" corp.")
        )
    {
        return true;
    }

    // Check for multi-word department/institutional patterns, but be more specific
    if trimmed.contains(" department of ")
        || trimmed.contains(" school of ")
        || trimmed.contains(" college of ")
        || trimmed.contains(" institute of ")
        || trimmed.contains(" university of ")
        || trimmed.contains(" medical center")
        || (trimmed.contains(" hospital") && !trimmed.contains("st.") && !trimmed.contains("saint"))
    {
        return true;
    }

    false
}

/// Extract text content from an HTML element
pub fn get_text(parser: &Parser, tag: &HTMLTag) -> String {
    let mut text_parts = Vec::new();

    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            text_parts.push(get_text(parser, child_tag));
        } else if let Some(text) = child.as_raw()
            && let Some(text_str) = text.try_as_utf8_str()
        {
            text_parts.push(decode_html_entities(text_str));
        }
    }

    text_parts.join(" ").trim().to_string()
}

/// Get text content excluding superscript elements (for author names)
pub fn get_text_excluding_superscripts(parser: &Parser, tag: &HTMLTag) -> String {
    let mut text_parts = Vec::new();

    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            let child_class = child_tag
                .attributes()
                .class()
                .map(|cls| cls.as_utf8_str())
                .unwrap_or_default();

            // Skip superscript elements
            if child_tag.name().as_utf8_str() == "sup" || child_class.contains("ltx_sup") {
                continue;
            }

            text_parts.push(get_text_excluding_superscripts(parser, child_tag));
        } else if let Some(text) = child.as_raw()
            && let Some(text_str) = text.try_as_utf8_str()
        {
            text_parts.push(decode_html_entities(text_str));
        }
    }

    text_parts.join(" ").trim().to_string()
}

/// Decode the root <article> element into aa Stencila [`Article`]
fn decode_article(parser: &Parser, article: &HTMLTag, context: &mut ArxivDecodeContext) -> Article {
    let mut title = None;
    let mut authors = Vec::new();
    let mut abstract_ = None;
    let mut references = Vec::new();
    let mut content = Vec::new();

    for child in article
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(tag) = child.as_tag() {
            let tag_name = tag.name().as_utf8_str();
            let class = get_class(tag);

            match tag_name.as_ref() {
                "h1" if class.contains("ltx_title_document") => {
                    title = Some(decode_inlines(parser, tag, context));
                }
                "div" if class.contains("ltx_authors") => {
                    authors = decode_authors(parser, tag);
                }
                "div" if class.contains("ltx_abstract") => {
                    abstract_ = Some(decode_abstract(parser, tag, context));
                }
                "section" if class.contains("ltx_bibliography") => {
                    references = decode_bibliography(parser, tag, context);
                }
                _ => {
                    content.append(&mut decode_blocks(parser, tag, context));
                }
            }
        }
    }

    if references.is_empty()
        && let Some(tag) = article
            .query_selector(parser, ".ltx_bibliography")
            .and_then(|mut query| query.next())
            .and_then(|node_handle| node_handle.get(parser))
            .and_then(|node| node.as_tag())
    {
        references = decode_bibliography(parser, tag, context);
    }

    Article {
        title,
        authors: (!authors.is_empty()).then_some(authors),
        r#abstract: abstract_,
        references: (!references.is_empty()).then_some(references),
        content,
        ..Default::default()
    }
}

/// Extract plain text from a vector of inlines
pub fn extract_text_from_inlines(inlines: &[Inline]) -> String {
    inlines
        .iter()
        .filter_map(|inline| match inline {
            Inline::Text(text) => Some(text.value.to_string()),
            _ => None,
        })
        .collect::<Vec<String>>()
        .join("")
}

/// Decode author information from div.ltx_authors
fn decode_authors(parser: &Parser, tag: &HTMLTag) -> Vec<Author> {
    let mut authors = Vec::new();

    // Look for ltx_creator elements within the authors div
    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            let child_class = child_tag
                .attributes()
                .class()
                .map(|cls| cls.as_utf8_str())
                .unwrap_or_default();

            if child_class.contains("ltx_creator") {
                authors.append(&mut decode_author_from_creator(parser, child_tag));
            }
        }
    }

    // If no individual creators found, fall back to extracting all text and parsing
    if authors.is_empty() {
        let full_text = get_text_excluding_superscripts(parser, tag);
        authors = decode_authors_from_text(&full_text);
    }

    authors
}

/// Decode a ltx_personname element that may contain authors and affiliations separated by <br> tags
///
/// This function handles the common LaTeXML pattern where author names are listed first,
/// followed by <br> tags and then affiliation information with superscript numbers.
fn decode_personname_element(parser: &Parser, tag: &HTMLTag) -> Vec<Author> {
    let mut authors = Vec::new();
    let mut current_text_parts = Vec::new();
    let mut found_first_br = false;
    let mut collecting_authors = true;

    // Get full text content for context in filtering structured emails
    let full_text = get_text_excluding_superscripts(parser, tag);

    // Iterate through child nodes to separate author names from affiliations
    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            let tag_name = child_tag.name().as_utf8_str().to_lowercase();

            if tag_name == "br" {
                if !found_first_br && collecting_authors && !current_text_parts.is_empty() {
                    // First <br> likely separates authors from affiliations
                    let author_text = current_text_parts.join(" ");
                    let mut parsed_authors =
                        decode_authors_from_text_with_context(&author_text, &full_text);
                    authors.append(&mut parsed_authors);
                    current_text_parts.clear();
                    found_first_br = true;
                    collecting_authors = false; // Stop collecting after first BR
                }
                // Subsequent <br> tags separate affiliation lines - we skip these
            } else if collecting_authors {
                // Only collect text before the first <br> for author parsing
                let element_text = get_text_excluding_superscripts(parser, child_tag);
                if !element_text.trim().is_empty() {
                    // Filter out obvious math symbols and formatting artifacts
                    let cleaned_text = clean_author_text(&element_text);
                    if !cleaned_text.is_empty() {
                        current_text_parts.push(cleaned_text);
                    }
                }
            }
            // After first <br>, we ignore content as it's likely affiliation info
        } else if collecting_authors
            && let Some(text) = child.as_raw()
            && let Some(text_str) = text.try_as_utf8_str()
        {
            let decoded_text = decode_html_entities(text_str).trim().to_string();
            if !decoded_text.is_empty() {
                let cleaned_text = clean_author_text(&decoded_text);
                if !cleaned_text.is_empty() {
                    current_text_parts.push(cleaned_text);
                }
            }
        }
    }

    // Process any remaining author parts (in case there was no <br>)
    if collecting_authors && !current_text_parts.is_empty() {
        let author_text = current_text_parts.join(" ");
        let mut parsed_authors = decode_authors_from_text_with_context(&author_text, &full_text);
        authors.append(&mut parsed_authors);
    }

    // If no authors found through structure parsing, fall back to full text parsing
    // but only use content before the first <br> or first superscripted number
    if authors.is_empty() {
        let full_text = get_text_excluding_superscripts(parser, tag);
        let author_portion = extract_author_portion_from_mixed_content(&full_text);
        authors = decode_authors_from_text(&author_portion);
    }

    authors
}

/// Clean author text by removing math symbols and formatting artifacts
fn clean_author_text(text: &str) -> String {
    text.replace("\\\\", "") // Remove LaTeX line breaks
        .replace("&", "and") // Convert & to 'and' for names like "Texas A&M"
        .trim()
        .to_string()
}

/// Extract the author portion from mixed content (authors + affiliations)
///
/// This function attempts to identify where author names end and affiliations begin
/// by looking for common patterns like numbered affiliations or institutional keywords.
fn extract_author_portion_from_mixed_content(text: &str) -> String {
    // Split by common affiliation indicators
    let affiliation_indicators = [
        "Department of",
        "University of",
        "Institute of",
        "College of",
        "School of",
        "Center",
        "Centre",
        "Laboratory",
        "Lab",
        // Add numbered superscript patterns
        "1Department",
        "2Department",
        "1University",
        "2University",
        "1Institute",
        "2Institute",
        "1College",
        "2College",
    ];

    let mut author_portion = text;

    // Find the earliest occurrence of any affiliation indicator
    let mut earliest_pos = text.len();
    for indicator in &affiliation_indicators {
        if let Some(pos) = text.find(indicator)
            && pos < earliest_pos
        {
            earliest_pos = pos;
        }
    }

    // If we found an affiliation indicator, cut the text there
    if earliest_pos < text.len() {
        author_portion = &text[..earliest_pos];
    }

    // Additional cleanup: remove trailing numbers and punctuation that might be affiliation markers
    author_portion
        .trim()
        .trim_end_matches(char::is_numeric)
        .trim_end_matches([',', ';', ':', '.'])
        .trim()
        .to_string()
}

/// Decode authors from a span.ltx_creator element
fn decode_author_from_creator(parser: &Parser, tag: &HTMLTag) -> Vec<Author> {
    // Look for ltx_personname within the creator
    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            let child_class = child_tag
                .attributes()
                .class()
                .map(|cls| cls.as_utf8_str())
                .unwrap_or_default();

            if child_class.contains("ltx_personname") {
                // Parse authors and affiliations from the personname element
                return decode_personname_element(parser, child_tag);
            }
        }
    }

    // Fallback: extract all text from creator
    let creator_text = get_text_excluding_superscripts(parser, tag);
    decode_authors_from_text(&creator_text)
}

/// Parse author names from text with full context for better filtering
///
/// This function is similar to `decode_authors_from_text` but accepts additional
/// context to help detect patterns like structured email addresses that should
/// not be parsed as individual author names.
fn decode_authors_from_text_with_context(text: &str, full_context: &str) -> Vec<Author> {
    let text = text.trim().trim_end_matches(['.', ',', ';']);

    // Try standard separators first (comma, ampersand, "and", newlines)
    static SPLIT_BY: LazyLock<Regex> =
        LazyLock::new(|| Regex::new(r",|&|\band\b|\n").expect("invalid regex"));
    let standard_parts: Vec<&str> = SPLIT_BY.split(text).collect();

    // If standard separators worked, process each part
    if standard_parts.len() > 1 {
        let authors: Vec<String> = standard_parts
            .into_iter()
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            // Filter out common non-author content found in LaTeXML output
            .filter(|s| !is_email_address(s)) // Remove email addresses
            .filter(|s| !is_organization_name(s)) // Remove institutional names
            .filter(|s| !is_email_username_part(s, full_context)) // Remove email username components
            .map(|s| s.to_string())
            .collect();

        // Convert each name string to a Person using the human_name parser
        return authors
            .into_iter()
            .map(|author| Person::from_str(&author).unwrap_or_default())
            .map(Author::Person)
            .collect();
    }

    // Fallback: try parsing superscript-separated names (common LaTeX pattern)
    // Pattern: "FirstName LastName 1  NextFirst NextLast 1  ..."
    if let Some(sup_authors) = parse_superscript_separated_authors(text)
        && sup_authors.len() > 1
    {
        return sup_authors;
    }

    // Single author fallback: validate it's not a non-author string
    let cleaned = text.trim();
    if !cleaned.is_empty()
        && !is_email_address(cleaned)
        && !is_organization_name(cleaned)
        && !is_email_username_part(cleaned, full_context)
        && let Ok(person) = Person::from_str(cleaned)
    {
        return vec![Author::Person(person)];
    }

    Vec::new()
}

/// Parse author names from text, filtering out non-author content
///
/// LaTeXML author sections often mix author names with email addresses,
/// institutional affiliations, and other metadata. This function attempts
/// to extract only the actual author names using multiple parsing strategies.
///
/// # Parsing Strategies
/// 1. Standard separators: commas, ampersands, "and", newlines
/// 2. Superscript-separated names (common LaTeX pattern)
/// 3. Single author fallback with content filtering
pub fn decode_authors_from_text(text: &str) -> Vec<Author> {
    let text = text.trim().trim_end_matches(['.', ',', ';']);

    // Try standard separators first (comma, ampersand, "and", newlines)
    static SPLIT_BY: LazyLock<Regex> =
        LazyLock::new(|| Regex::new(r",|&|\band\b|\n").expect("invalid regex"));
    let standard_parts: Vec<&str> = SPLIT_BY.split(text).collect();

    // If standard separators worked, process each part
    if standard_parts.len() > 1 {
        let authors: Vec<String> = standard_parts
            .into_iter()
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            // Filter out common non-author content found in LaTeXML output
            .filter(|s| !is_email_address(s)) // Remove email addresses
            .filter(|s| !is_organization_name(s)) // Remove institutional names
            .filter(|s| !is_email_username_part(s, text)) // Remove email username components
            .map(|s| s.to_string())
            .collect();

        // Convert each name string to a Person using the human_name parser
        return authors
            .into_iter()
            .map(|author| Person::from_str(&author).unwrap_or_default())
            .map(Author::Person)
            .collect();
    }

    // Fallback: try parsing superscript-separated names (common LaTeX pattern)
    // Pattern: "FirstName LastName 1  NextFirst NextLast 1  ..."
    if let Some(sup_authors) = parse_superscript_separated_authors(text)
        && sup_authors.len() > 1
    {
        return sup_authors;
    }

    // Last resort: treat as single author if it passes our filters
    let trimmed = text.trim();
    if !trimmed.is_empty()
        && !is_email_address(trimmed)
        && !is_organization_name(trimmed)
        && let Ok(person) = Person::from_str(trimmed)
    {
        return vec![Author::Person(person)];
    }

    vec![]
}

/// Try to parse authors separated by superscript numbers
fn parse_superscript_separated_authors(text: &str) -> Option<Vec<Author>> {
    // Pattern to detect: "Name1 Number  Name2 Number  ..."
    // Look for digit followed by 2+ spaces pattern
    static SUP_PATTERN: LazyLock<Regex> =
        LazyLock::new(|| Regex::new(r"\d+\s{2,}").expect("invalid regex"));

    // Check if text contains the pattern
    if !SUP_PATTERN.is_match(text) {
        return None;
    }

    // Split by "digit + multiple spaces"
    let parts: Vec<&str> = SUP_PATTERN.split(text).collect();

    // Clean up each part and try to parse as author
    let mut authors = Vec::new();
    for part in parts {
        let cleaned = part
            .trim()
            .trim_end_matches(char::is_numeric) // Remove trailing numbers
            .trim();

        if !cleaned.is_empty()
            && !is_email_address(cleaned)
            && !is_organization_name(cleaned)
            && let Ok(person) = Person::from_str(cleaned)
        {
            authors.push(Author::Person(person));
        }
    }

    if authors.len() > 1 {
        Some(authors)
    } else {
        None
    }
}

/// Decode abstract from div.ltx_abstract
fn decode_abstract(parser: &Parser, tag: &HTMLTag, context: &mut ArxivDecodeContext) -> Vec<Block> {
    decode_blocks(parser, tag, context)
}

/// Extract label and other inlines from a tag
pub fn extract_label_and_inlines(
    parser: &Parser,
    tag: &HTMLTag,
    context: &mut ArxivDecodeContext,
) -> (Option<String>, Vec<Inline>) {
    let mut label = None;
    let mut content_parts = Vec::new();

    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            let child_class = child_tag
                .attributes()
                .class()
                .map(|cls| cls.as_utf8_str())
                .unwrap_or_default();

            if child_tag.name().as_utf8_str() == "span" && child_class.contains("ltx_tag") {
                let label_text = get_text(parser, child_tag).trim().to_string();
                if !label_text.is_empty() {
                    label = Some(label_text);
                }
            } else {
                content_parts.extend(decode_inlines(parser, child_tag, context));
            }
        } else if let Some(text) = child.as_raw() {
            let text_content = text.try_as_utf8_str().unwrap_or_default().trim();
            if !text_content.is_empty() {
                content_parts.push(t(text_content));
            }
        }
    }

    (label, content_parts)
}

/// Extract both LaTeX code and MathML content from a math element
pub fn extract_latex_and_mathml(parser: &Parser, tag: &HTMLTag) -> (String, String) {
    let mathml = if tag.name().as_utf8_str() == "math" {
        tag.outer_html(parser)
    } else {
        // For equation tables, look for math elements within
        let found_math = find_math_elements(parser, tag);

        if found_math.is_empty() {
            // If no MathML found, wrap content in math tags
            let inner_html = tag.inner_html(parser);
            if inner_html.trim().is_empty() {
                String::new()
            } else {
                format!("<math>{inner_html}</math>")
            }
        } else {
            found_math
        }
    };

    let latex = get_attr(tag, "alttext")
        .filter(|s| !s.is_empty())
        .or_else(|| {
            let annotations_latex = extract_latex_from_annotations(parser, tag);
            if annotations_latex.is_empty() {
                None
            } else {
                Some(annotations_latex)
            }
        })
        .unwrap_or_else(|| {
            if mathml.is_empty() {
                String::new()
            } else {
                "\\text{Math content}".to_string()
            }
        });

    (latex, mathml)
}

/// Find math elements within a container
fn find_math_elements(parser: &Parser, tag: &HTMLTag) -> String {
    tag.children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
        .filter_map(|child| child.as_tag())
        .filter_map(|child_tag| {
            if child_tag.name().as_utf8_str() == "math" {
                Some(child_tag.outer_html(parser))
            } else {
                // Recursively search for math elements
                let nested_math = find_math_elements(parser, child_tag);
                if nested_math.is_empty() {
                    None
                } else {
                    Some(nested_math)
                }
            }
        })
        .collect::<Vec<_>>()
        .join("")
}

/// Extract LaTeX code from annotation elements within MathML
fn extract_latex_from_annotations(parser: &Parser, tag: &HTMLTag) -> String {
    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            // Look for annotation elements with LaTeX
            if child_tag.name().as_utf8_str() == "annotation"
                && let Some(encoding) = child_tag.attributes().get("encoding").flatten()
                && encoding.as_utf8_str().contains("tex")
            {
                return get_text(parser, child_tag);
            }

            // Recursively search in child elements
            let nested_latex = extract_latex_from_annotations(parser, child_tag);
            if !nested_latex.is_empty() {
                return nested_latex;
            }
        }
    }

    String::new()
}

/// Decode bibliography section and extract Reference objects
fn decode_bibliography(
    parser: &Parser,
    tag: &HTMLTag,
    _context: &mut ArxivDecodeContext,
) -> Vec<Reference> {
    let mut references = Vec::new();

    // Look for ul.ltx_biblist within the bibliography section
    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag() {
            let child_class = child_tag
                .attributes()
                .class()
                .map(|cls| cls.as_utf8_str())
                .unwrap_or_default();

            if child_tag.name().as_utf8_str() == "ul" && child_class.contains("ltx_biblist") {
                // Process each li.ltx_bibitem as a reference
                for bibitem in child_tag
                    .children()
                    .top()
                    .iter()
                    .flat_map(|h| h.get(parser))
                {
                    if let Some(item_tag) = bibitem.as_tag() {
                        let item_class = item_tag
                            .attributes()
                            .class()
                            .map(|cls| cls.as_utf8_str())
                            .unwrap_or_default();

                        if item_tag.name().as_utf8_str() == "li"
                            && item_class.contains("ltx_bibitem")
                        {
                            let reference = decode_reference(parser, item_tag);
                            references.push(reference);
                        }
                    }
                }
            }
        }
    }

    references
}

/// Decode a single bibliography item into a Reference
fn decode_reference(parser: &Parser, tag: &HTMLTag) -> Reference {
    // Extract id from the li element
    let id = tag
        .attributes()
        .get("id")
        .flatten()
        .map(|bytes| bytes.as_utf8_str().to_string());

    // Collect the text from all ltx_bibblock spans
    let mut reference = String::new();
    for child in tag
        .children()
        .top()
        .iter()
        .flat_map(|handle| handle.get(parser))
    {
        if let Some(child_tag) = child.as_tag()
            && child_tag.name().as_utf8_str() == "span"
        {
            let class = get_class(child_tag);
            if class.contains("ltx_bibblock") {
                let text = decode_html_entities(&child_tag.inner_text(parser));
                let text = text.trim();
                if !text.is_empty() {
                    reference.push_str(text);
                    reference.push(' ');
                }
            }
        }
    }
    let reference = reference.replace("\u{a0}", " ").replace("\n", " ");

    // Parse reference and give it the id
    Reference {
        id,
        ..text_to_reference(&reference)
    }
}
