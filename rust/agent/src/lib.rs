use common::{
    async_trait::async_trait,
    clap::{self, ValueEnum},
    eyre::{bail, Result},
    strum::Display,
};

// Export crates for the convenience of dependant crates
pub use common;

/// The type of input or output an agent can consume or generate
#[derive(Debug, Clone, PartialEq, Eq, ValueEnum, Display)]
#[strum(serialize_all = "lowercase")]
pub enum AgentIO {
    Text,
    Image,
    Audio,
    Video,
}

/// Options for the `Agent::generate_text` method
///
/// These options are based on those supported by various APIs
/// but each option is not necessarily supported by each `Agent`.
///
/// Currently, they are based on those documented for `ollama`
/// documented at https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values
#[derive(Default)]
pub struct GenerateTextOptions {
    /// The system prompt to use
    pub system_prompt: Option<String>,

    /// Enable Mirostat sampling for controlling perplexity
    ///
    /// Default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0                                                                                                                                   
    pub mirostat: Option<u8>,

    /// Influences how quickly the algorithm responds to feedback from the generated text.
    ///
    /// A lower learning rate will result in slower adjustments, while a higher learning rate will
    /// make the algorithm more responsive. Default: 0.1                         
    pub mirostat_eta: Option<f32>,

    /// Controls the balance between coherence and diversity of the output.
    ///
    /// A lower value will result in more focused and coherent text. Default: 5.0                                                                                                       
    pub mirostat_tau: Option<f32>,

    /// Sets the size of the context window used to generate the next token.
    ///
    /// Default: 2048                                                                                                                                                                  
    pub num_ctx: Option<u32>,

    /// The number of GQA groups in the transformer layer.
    ///
    /// Required for some models, for example it is 8 for llama2:70b                                                                                                                                       
    pub num_gqa: Option<u32>,

    /// The number of layers to send to the GPU(s).
    ///
    /// On macOS it defaults to 1 to enable metal support, 0 to disable.                                                                                                                                          
    pub num_gpu: Option<u32>,

    /// Sets the number of threads to use during computation.
    ///
    /// By default, Ollama will detect this for optimal performance. It is recommended to set this
    /// value to the number of physical CPU cores your system has (as opposed to the logical number of cores).    
    pub num_thread: Option<u32>,

    /// Sets how far back for the model to look back to prevent repetition.
    ///
    /// Default: 64, 0 = disabled, -1 = num_ctx                                                                                                                                         
    pub repeat_last_n: Option<i32>,

    /// Sets how strongly to penalize repetitions.
    ///
    /// A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.
    /// Default: 1.1                                                                   
    pub repeat_penalty: Option<f32>,

    /// The temperature of the model.
    ///
    /// Increasing the temperature will make the model answer more creatively.
    /// Default: 0.8                                                                                                                                   
    pub temperature: Option<f32>,

    /// Sets the random number seed to use for generation.
    ///
    /// Setting this to a specific number will make the model generate the same text for the same prompt.
    /// Default: 0                                                                                        
    pub seed: Option<i32>,

    /// Sets the stop sequences to use.
    ///
    /// When this pattern is encountered the LLM will stop generating text and return.
    /// Multiple stop patterns may be set by specifying multiple separate `stop` parameters in a modelfile.                                    
    pub stop: Option<String>,

    /// Tail free sampling is used to reduce the impact of less probable tokens from the output.
    ///
    /// A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting.
    /// Default: 1                                             
    pub tfs_z: Option<f32>,

    /// Maximum number of tokens to predict when generating text.
    ///
    /// Default: 128, -1 = infinite generation, -2 = fill context                                                                                                                                      
    pub num_predict: Option<i32>,

    /// Reduces the probability of generating nonsense.
    ///
    /// A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative.
    /// Default: 40                                                                      
    pub top_k: Option<u32>,

    /// Works together with top-k.
    ///
    /// A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more
    /// focused and conservative text. Default: 0.9                                                               
    pub top_p: Option<f32>,
}

/// Options for the `Agent::generate_image` method
pub struct GenerateImageOptions {
    // TODO
}

/// An AI agent
///
/// Provides a common, shared interface for the various AI models
/// and APIs used. Agent implementations should override
/// `supports_generating` and other methods.
#[async_trait]
pub trait Agent: Sync + Send {
    /**
     * Get the fully qualified name of the agent
     */
    fn name(&self) -> String;

    /**
     * Get a list of input types this agent supports
     */
    fn supported_inputs(&self) -> &[AgentIO] {
        &[]
    }

    /**
     * Get a list of output types this agent supports
     */
    fn supported_outputs(&self) -> &[AgentIO] {
        &[]
    }

    /**
     * Get a list of output types this agent supports
     */
    fn supports_from_to(&self, input: AgentIO, output: AgentIO) -> bool {
        self.supported_inputs().contains(&input) && self.supported_outputs().contains(&output)
    }

    /**
     * Generate text in response to an instruction
     */
    #[allow(unused)]
    async fn text_to_text(
        &self,
        instruction: &str,
        options: Option<GenerateTextOptions>,
    ) -> Result<String> {
        bail!("Text generation is not supported")
    }

    /**
     * Generate image in response to an instruction
     */
    #[allow(unused)]
    async fn text_to_image(
        &self,
        instruction: &str,
        options: Option<GenerateImageOptions>,
    ) -> Result<String> {
        bail!("Image generation is not supported")
    }
}
